{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In Convolutional Neural Networks, we do not deal with rows and columns. We deal with pixels and hence we cannot folow the folder and file structure that we use for other Deep Learning algorithms.\n",
    "    \n",
    "    We will use Keras to import the images. We do this by preparing a special structure for the dataset. We split the dataset into two seperate folders - Training & Test set folders. We also need a simple way to diffrentiate the class labels for the images.\n",
    "    \n",
    "    The simple trick to use here is to make diffrent folders for the diffrent class labels of the images. This is to help Keras diffrentiate between the class labels.\n",
    "    \n",
    "    The folder structure is the data preprocessing step in CNN.\n",
    "    \n",
    "    Feature Scaling is a compulsory step in Deep Learning. We will apply Feature Scaling for CNN before we fit the CNN to the images.\n",
    "    \n",
    "    The take-away here is that the data prepeocessing is not done using Python but is done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN - Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries and packages from the Keras module\n",
    "# Sequential package will be used to initialise the Neural Network\n",
    "# Convolution2D package will be used to make the first step in building the CNN - adding the Convolution layers\n",
    "# Conv2D is used in case of classifying images as they are in 2D\n",
    "# MaxPooling2D package will be used to make the second step in building the CNN - adding our Pooling layers\n",
    "# Flatten package will be used to make the third step in building te CNN -\n",
    "# converting the pooled feature maps created through Convolution & Pooling into a large feature vector which then \n",
    "# becomes the input for the Fully Connected Layers of the CNN\n",
    "# Dense package will be used to make the final step in building the CNN - add the Fully Connected Layers in a classic ANN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the CNN - Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "# Create a classifier object of the Sequential class\n",
    "\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution - Adding the first Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"add\" method \n",
    "# Use the \"Conv2D\" function \n",
    "# \"32\" denotes the number of filters/feature detectors applied on the input image - An optimum number to use\n",
    "# to get the same number of feature maps - The number of filters = The number of Feature Maps created\n",
    "# \"(3, 3)\" denotes the number of rows & columns in the Convolution Kernel/Feature Detector\n",
    "# \"32, (3, 3)\" denotes that 32 Feature Maps with 3 rows and 3 columns(3*3) each will be created\n",
    "# \"input_shape\" denotes the shape of the input image - Convert all the images into a fixed/single format\n",
    "# This conversion will be done during the image preprocessing step after we build the CNN \n",
    "# and before we fit the CNN to the images\n",
    "# The expected format needs to be specified into which the images will be converted\n",
    "# The number of channels will be \"1\" for black & white images\n",
    "# The number of channels will be \"3\" for colour images\n",
    "# (64, 64, 3) 3 denotes the number of channels, \n",
    "# 64, 64 denotes the dimensions of the array - 64*64 pixels of colour images\n",
    "# The order of the parameters in the \"input_shape\" is important.\n",
    "# We are using Tensorflow backend - Hence the dimensions array will be the first parameter passed\n",
    "# The second parameter passed will be the number of channels\n",
    "# Classification of images is a non-linear problem and we need to have non-lineaity \n",
    "# \"Activation\" denotes the activation function used in the CNN \n",
    "# Rectifier(ReLU) is the best option \n",
    "# We can get some negative pixels in the feature maps \n",
    "# The negative pixels need to be removed in order to have non-linearity in the CNN\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling - Max pooling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply Max Pooling to reduce the size of feature maps which \n",
    "# in turn will reduce the number of nodes obtained in the next step\n",
    "# This will ensure that the CNN model will not be too complex and compute intensive but not affect the performance \n",
    "# Max Pooling ensures that by taking the maximum of the 2*2 sub-tables of the feature map, \n",
    "# information is retained and we are keeping track of the part of the image that contain the high numbers corresponding \n",
    "# to where the feature detectors detected some features in the input image\n",
    "# \"pool_size\" denotes the size of the sub-table used to slide \n",
    "# across the feature map to take the maximum in each sub table\n",
    "# (2, 2) is the general parameter passed for Max Pooling - Helps to keep the information \n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten - Flatten the feature map into a single vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Flattening consists of putting all the numbers in the cells derived in the Feature Maps into one single vector\n",
    "\n",
    "    We do not lose the facial structure of the images by creating a single vector because we extracted the spatial structure information by creating the Feature Maps in the Convolution step. We do this by applying the feature detectors on the input image which give the high numbers that represent the spatial structure of the images - the high numbers are associated to specific features in the input image.\n",
    "    \n",
    "    We do not directly take all the pixels of the input image and flatten them into one single vector without applying the previous steps - Convolution & max Pooling. If we directly flatten the pixels into the single vector, then each node of the huge vector will represent one pixel of the image independently of the pixels that are around it. We only get information of the pixel itself and not of how this pixel is spatially connected to the other pixels around it.\n",
    "    \n",
    "    If we apply the Convolution and Max Pooling step to create all the reduced size Feature Maps and flatten all these Feature Maps into a single vector, then each node of the high vector will represent the information of a specific feature of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Flatten() function\n",
    "\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection - Adding fully Connected Layers to the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This step is required because we have managed to convert the 2D images into a 1D vector\n",
    "* The 1D vector contains some information of te spatial structure & pixel patterns of the image\n",
    "* We will use this input vector as the input layer to build the ANN consisting of fully connected layers\n",
    "* We do this because ANN is a great classifier for non-linear problems like image classification\n",
    "* We will then create a hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hidden layer\n",
    "# \"units\" denote the number of nodes in the hidden layer. \n",
    "# The number of input nodes in this case is very large and not possible to count\n",
    "# We will choose \"128\" as an optimum number and this optimum number is derived by several experimentations\n",
    "# It is a common practice to go with numbers ending with power of 2\n",
    "# The nodes in the hidden layer are like neurons that need to be activated \n",
    "# based on how much they can pass on the signal\n",
    "# Use ReLU as the activation function\n",
    "\n",
    "\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer - Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output layer\n",
    "# \"units\" will be 1 as we need only one node\n",
    "# Use \"Sigmoid\" as the activation function for the output as the outcome is a binary one\n",
    "\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the CNN with optimiser - Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "# Optimiser refers to the algorithm to obtain optimised values for weights \n",
    "# We will be using one of the most efficient stochastic gradient descent algorithms named \"adam\"\n",
    "# The \"adam\" optimiser is always a safe choice because it is very powerful \n",
    "# and always perfoms relevant updates of the weights\n",
    "# \"loss\" function denotes the cost function which gives the error in prediction accuracy \n",
    "# For Linear type of problems, it is the squared sum of differences (OLS)\n",
    "# For classification type of problems, it is the logarithmic function \n",
    "# Since in this case, the output is of binary type, we choose the binary crossentropy for \"loss\"\n",
    "# \"metrics\" denotes the evaluation criteria to be chosen to evaluate the model performance. \n",
    "# In case of linear problems, the metric used is \"Mean Squared Error(MSE)\"\n",
    "# In case of classification problems, the metric used is \"accuracy\"\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing \n",
    "## Train the CNN\n",
    "## Test the CNN - Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The image preprocessing is a necessary step required to be sone before training the CNN. This process is called image augmentation. It consists of preprocessing the images to prevent overfitting. If we do not do it, then we will most probably get a very high accuarcy in the training set but a low accuracy in the test set.\n",
    "    \n",
    "    We will do the image preprocessing in one step using the Keras documentation.\n",
    "    \n",
    "    https://keras.io/api/preprocessing/image/\n",
    "    \n",
    "    The first function that will be used is the \"ImageDataGenerator\" which will be used to generate the image augmentation. One of the situations leading to overfitting is when we have few data to train the model. In such situations, the model finds some correlations in few observations of the training set but fails to generalize these correlations on new observations. \n",
    "    \n",
    "    In images classification, a huge data of images is required to find and generalize the correlations because in Computer Vision, the ML model does not seem to need to find correlations between the independent variables and some dependent variables. It actullay needs to find some patterns in the pixels and to do this, a lot of images are required. \n",
    "    \n",
    "    So, in cases where the dataset of images available are not very huge, image augmentation may be the answer to achieve a high level of accuracy in the prediction. It will create many batches of the images. It will then apply random transformations on each batch on a random selection of images by rotating, flipping or even shearing them. This will result in the generation of many more diverse images during training in the batches and this means more images to train on. Image augmentation, put simply augments the training images. Hence the name. Since the transformations are random, the model will never find the same picture across the batches. \n",
    "\n",
    "    Image Augmentation is a technique that allows to enrich the dataset, specifically, the training set without adding more images and that allows to get good performance results with little or no overfitting, even with a small dataset of images.\n",
    "\n",
    "    We will be using the code from \".flow_from_directory(directory)\" method for the model that we will be training because we require the code to be structured in this specific way so that the classes can be well identified in the separate folders. \n",
    "    \n",
    "    The section of code available here - the \"fit_generator\" method  will not only fit the training dataset to the CNN but will also test the performance of the model on some new observations which will be the test set.\n",
    "    \n",
    "    Feature Scaling is always compulsory and is taken care using the \"rescale\" method which corresponds to the feature scaling part of the data preprocessing step discussed above. There are other transformations like shear range coresponding to shearing. \n",
    "    \n",
    "    Shearing is a geometrical transformation or transvection where the pixels are moved to a fixed direction over a proportional distance from a line that is parallel to the direction that they are moving to. \n",
    "    \n",
    "    There are other transformations like zoom range - which is a random zoom that can be applied on the images. The horizontal flip flips the image horizontally. The vertical flip is not used here. \n",
    "    \n",
    "    We will be applying the image augmentation on the training and test dataset and then we use the \"ImageDataGenerator\" function to rescale only the images from the test set. \n",
    "    \n",
    "    The \"train_generator\"(training_set) and the \"validation_generator\"(test_set) actually create the training and test sets. The training set created here will contain all the augmented images extracted from the \"ImageDataGenerator\" and it will also create the test set that will be used to evalaute the model performance and prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ImageDataGenerator class from the image preprocessing module of the Keras library\n",
    "# Create a \"train_datagen\" object of the ImageDataGenerator class\n",
    "# Create a \"test_datagen\" object of the ImageDataGenerator class\n",
    "# Apply a \"rescale\" value betwween 0 and 1 for the pixel values\n",
    "# Pixels take value between 0 and 255 and by rescaling them using 1./255, all pixels will be between 0 and 1\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder from which the data needs to be extracted for both the training and test sets\n",
    "# \"target_size\" denotes the size of the images expected in the CNN model\n",
    "# As we already specified the dimension (64, 64) we will keep the same here\n",
    "# \"batch_size\" denotes the size of the batches in which some random samples of the images will be included\n",
    "# It contains the number of images that will flow through the CNN after which the weights will be updated\n",
    "# \"class-mode\" denotes if the class of the dependent variable has two categories or more than two categories\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder from which the data needs to be extracted for both the training and test sets\n",
    "# \"target_size\" denotes the size of the images expected in the CNN model\n",
    "# As we already specified the dimension (64, 64) we will keep the same here\n",
    "# \"batch_size\" denotes the size of the batches in which some random samples of the images will be included\n",
    "# It contains the number of images that will flow through the CNN after which the weights will be updated\n",
    "# \"class-mode\" denotes if the class of the dependent variable has two categories or more than two categories\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rameshveer/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=5, validation_data=<keras.pre..., validation_steps=2000, steps_per_epoch=250)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 327s 1s/step - loss: 0.5384 - accuracy: 0.7239 - val_loss: 0.4500 - val_accuracy: 0.7342\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 367s 1s/step - loss: 0.5239 - accuracy: 0.7362 - val_loss: 0.6378 - val_accuracy: 0.7020\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 539s 2s/step - loss: 0.5039 - accuracy: 0.7495 - val_loss: 0.4611 - val_accuracy: 0.7360\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 368s 1s/step - loss: 0.4918 - accuracy: 0.7605 - val_loss: 0.4214 - val_accuracy: 0.7189\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 322s 1s/step - loss: 0.4832 - accuracy: 0.7638 - val_loss: 0.4329 - val_accuracy: 0.7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ffd58358d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the section where we fit the training set into the CNN model\n",
    "# The model performance and the prediction accuarcy is also tested here\n",
    "# \"samples_per_epoch\" will be the number of images in the training set \n",
    "# All the observations of the trainig set pass through the CNN during each epoch\n",
    "# \"epochs\" is the number of epochs to run to train the CNN\n",
    "# \"validation_data\" is the data where the model is evaluated which is the test set\n",
    "# \"validation_steps\" is the number of images in the test set\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         epochs = 5,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have obtained an accuracy of 86.05% for the training set and 76% for the test set. It is not very bad but it is not very good either.\n",
    "    \n",
    "* We first obtained around 86.05% accuracy on the training set which is not bad but as data scientists, we are more interested in the accuracy of the test set which is around 76% and the difference between the accuracy of the training set and accuracy of the test set to assess wherther there is overfitting or not. So, 76% accuracy on the test set is not bad. \n",
    "    \n",
    "* The accuracy for the test set which is 76% denotes that we get three correct predictions out of four which is not bad. But we do get a large difference between the accuracy on the training set and the test set. It does not necessarily signify any important overfitting but it does laeve a lot of room for improvement.\n",
    "    \n",
    "* There can be a siginificant increase in the accuracy on the test set. The difference between the accuracy of the test set and the training set can also be made smaller.\n",
    "    \n",
    "* The goal wpould be to increase the accuracy on the test set over 80% and decrease the difference between the accuracy on the training and test sets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
